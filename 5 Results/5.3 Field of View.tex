\section{Field of View}
\label{sec:Field of View}

% Better definition of Field of View

A significant contribution from Pop-Share \cite{Lagesse2021-PopShare} was the ability to accurately classify videos from various angles for similarity. Applying the methodology from Similarity of Simultaneous Observation (SSO) \cite{Wu2019-SSO} to compare videos, Pop-Share re-used the pre-processing procedure to convert videos into a byte-count array. Instead of comparing videos to capture network traffic, the application of SSO was altered to compare two videos to each other for similarity. In addition, Pop-Share expanded upon the distance measure implementation to support Fully Homomorphic Encryption, requiring a new set of algorithms that were simplified into basic arithmetic. 

% Training setup

The field-of-view experiment demonstrated that comparing two videos taken at the same time and place of the same subject could be accurately classified using an Artificial Neural Network (ANN). This machine-learning model was trained on the distance measure scores between two videos computing using three probability distribution functions: Kullback-Leibler Divergence (KLD), Bhattacharyya Coefficient (BC), and Cramer Distance (CD). For supervised training, the binary labeling system assigns a one when two scenes are identical, or the first comparison pair is labeled as one to train the model to match similar videos. Otherwise, a zero is assigned for all other comparison pairs, regardless of visual similarity.

Once the videos are aligned and pre-processed, we compute the distance measures of the byte-count arrays representing a one-second segment of each video. In the generated CSV file, each row contains the binary label, followed by all of the scores: KLD, BC, and CD. Since KLD is asymmetric, the scores will differ when comparing A to B versus B to A. We performed an exhaustive comparison of every video, as well as the inverse comparison, to account for asymmetric differences.

\input{5 Results/Tables/5.3 ANN System Compare}

The Multi-Layer Perceptron (MLP) classifier is part of the scikit-learn \cite{scikit-learn} library. The MLP Classifier used the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (lbfgs) solver to apply weights within the neural network. The model was trained with two hidden layers, each with 10 neurons, and used Rectified Linear Unit (relu), a popular activation function in deep learning. This is the same machine learning model used in SSO and Pop-Share. SSO and Pop-Share were trained for 15 hours using a Nexus 6p and a D-Link Wi-Fi camera (DCS-936L) fixed-motion cameras. The video data from the two cameras include video captures at different resolutions and different relative angles, such as 0, 90, and 180 degrees offset from each other. The videos were also taken at varying distances from each other, ranging from 1 to 25 meters away. In addition, the videos were taken from both an indoor and an outdoor environment with varying levels of motion and lighting conditions.

The Handheld model shown in Table \ref{table:ann_system_compare} was tested using 150 minutes of training data recorded with a Google Pixel 2, a Motorola Moto Z, a Lenovo Phab2 Pro, an LG Nexus 5, and a Huawei Nexus 6p. All phones captured video using h.264 with 3840x2160 resolution at 30 FPS with Optical Image Stabilization (OIS) enabled except the Nexus 5 and Phab2 Pro, which only support 1920x1080 resolution, and the Nexus 6p, which only has Electronic Image Stabilization (EIS). 

GhostPeerShare was trained and tested on a less diverse dataset of 100 minutes of raw videos, which included three twenty-minute videos of low movement featuring an individual sitting at his desk, denoted as \textit{office}, and two twenty-minute videos of high movement capturing an individual vacuuming his living room, denoted as \textit{vaccum} in Appendix Table \ref{table:appendix_ann}. The video data was recorded on fixed-motion phones: Pixel 3XL using h.264 with 1920x1080 resolution at 30 FPS and Samsung S9 using h.264 with 1280x720 resolution at 30 FPS. The phones were placed at different relative angles and varying distances. However, all videos were filmed indoors with relatively similar lighting. 

For a practical analysis, the ANN was trained on a high-movement scene and tested on a low-movement scene, achieving an Accuracy and Recall score of 98.05\%, a Precision Score of 96.14\%, and an F1 Score of 97.09\%. The lack of diversity within the training and testing dataset is likely a contributing factor to the lower scores for F1 and Precision. Recording more video data in various locations with a wide variety of devices and environments, the F1 and Precision scores may improve. This model demonstrates that GhostPeerShare consistently generates a unique representation of video data and can accurately predict the binary label given the three distance measure scores for a different dataset.
