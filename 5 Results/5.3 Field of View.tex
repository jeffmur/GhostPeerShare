\section{Field of View}
Borrowing the methodology from Pop-Share, we compare videos without revealing their content, manually labeling each row of similarity scores with a one or zero to train a supervised artificial neural network for classifying new scenes. The binary labeling system assigns a one when two scenes are similar or identical; otherwise, a zero is assigned. In Dart, we exhaustively compare the plaintext byte count arrays to generate a row of labels along with the Kullback-Leibler Divergence (KLD), Bhattacharyya Coefficient (BC), and Cramer Distance (CD) for comparison between each one-second segment of videos of the same length. Prior to comparison, both videos have been manually aligned and trimmed to ensure consistency.

\input{5 Results/Tables/5.3 ANN System Compare}

As shown in Table \ref{table:ann_system_compare}, we contribute GhostPeerShare to the previous analysis conducted in Pop-Share by employing supervised learning with an artificial neural network to accurately classify a binary score, representing the same or different scene based on the three distance measures. We trained and tested the model on a small dataset of 100 minutes of raw videos, which included three twenty-minute videos of low movement featuring an individual sitting at a desk, as well as two twenty-minute videos of high movement capturing an individual vacuuming his living room. The best results from all training and testing scenarios, detailed in Appendix Table \ref{table:appendix_ann}, achieved an accuracy and recall of 99.95\%, a precision of 100\%, and an F1 Score of 99.98\%. This performance was attained using a grid search, with training data based on the raw videos and testing conducted solely on comparisons of different scenes, where the label is exclusively zero.
